####################
#initialization
####################
beta.old = beta0 #initiliaze beta
n = dim(X)[1]
p = dim(X)[2]
#Newon-Raphson
for(k in 1:epochs){
eta.hat = X %*% beta.old
y.hat = c(exp(eta.hat) / (1+exp(eta.hat))  )
w = pmax(y.hat*(1-y.hat) , 1e-10)  #numerical tolerance
X.tilde = sqrt(w)*X
y.tilde = eta.hat * w + (y-y.hat)
beta.new = c(solve(crossprod(X.tilde) +lambda*diag(c(0,rep(1,p-1))) ,
crossprod(X,y.tilde)))
if(max(abs(beta.new-beta.old)) < tol){ #no more updates
break
}
beta.old=beta.new
}
eta.hat = X %*% beta.old
y.hat = c(exp(eta.hat) /(1+exp(eta.hat))  )
pred = as.numeric(y.hat > 0.5)
class.err = 1-sum(pred==y)/n
return( list(coefficients=beta.new , y.hat=y.hat, pred=pred, err=class.err) )
}
logreg.ridge.cv = function(y, X, lambdas, nfolds=5, plot=F){
n = dim(X)[1]
p = dim(X)[1]
num.lambda = length(lambdas)
beta.init = rep(0,p)
cv.results = rep(0,num.lambda)
for(i in 1:num.lambda){
groups = rep(1:nfolds, n/nfolds)
for( j in 1:nfolds){
train = groups!=j
test = groups==j
logreg_object = fit.logreg.ridge(y[train], X[train,] , lambda = lambdas[i],
epochs=10,tol=1e-3)
pred = logreg.predict(logreg_object, X[test,] ,y[test] )
cv.results[i] = cv.results[i] + pred$err/nfolds
}
}
if(plot){
plot(log(lambdas[i], cv.results, type='b',
xlab='log lambda', ylab='prediction error',
main=paste0(nfolds,"-fold CV results"))
)
}
lambda.min =lambdas[cv.results==min(cv.results)]
return(list(lambda.min=lambda.min,err=cv.results,lambda.vec=lambdas))
}
logreg.predict = function(logreg_object, X.new, y.new='None'){
beta = logreg_object[['coefficients']]
if(y.new[1] =='None'){
class.err='None'
n = dim(X.new)[1]
eta.hat = X.new %*% beta
y.hat = c(exp(eta.hat) / (1+exp(eta.hat)))
pred = as.numeric(y.hat>0.5)
}else{
n = length(y.new)
eta.hat = X.new %*% beta
y.hat = c( exp(eta.hat) ) / (1+exp(eta.hat))
pred = as.numeric(y.hat>0.5)
class.err = 1-sum(pred==y.new)/n
}
return( list(y.hat=y.hat, pred=pred, err=class.err) )
}
logreg.ridge.cv(y,x, c(0,0.3,0.5),plot=T)
help(log)
#' logreg.ridge.cv
#' This function works to perform cross-validation to select the best lambda
#' @param y the input y as response
#' @param X the input X to fit model
#' @param lambdas a vector specifies the lambda candidates
#' @param nfolds an int sepecific how many folds you want to split the data
#' @param plot this boolean variable specifies if you want to plot the result
logreg.ridge.cv = function(y, X, lambdas, nfolds=5, plot=F){
n = dim(X)[1]
p = dim(X)[1]
num.lambda = length(lambdas)
beta.init = rep(0,p)
cv.results = rep(0,num.lambda)
for(i in 1:num.lambda){
groups = rep(1:nfolds, n/nfolds)
for( j in 1:nfolds){
train = groups!=j
test = groups==j
logreg_object = logreg.ridge.fit(y[train], X[train,] , lambda = lambdas[i],
epochs=10,tol=1e-3)
pred = logreg.ridge.predict(logreg_object, X[test,] ,y[test] )
cv.results[i] = cv.results[i] + pred$err/nfolds
}
}
if(plot){
plot( log(lambdas[i]) , cv.results, type='b',
xlab='log lambda', ylab='prediction error',
main=paste0(nfolds,"-fold CV results")
)
}
lambda.min =lambdas[cv.results==min(cv.results)]
return(list(lambda.min=lambda.min,err=cv.results,lambda.vec=lambdas))
}
logreg.ridge.cv(y,x, c(0,0.3,0.5),plot=T)
logreg.ridge.fit = function(y, X, lambda, epochs=10,
tol=1e-5, beta0 = rep(0,dim(X)[2])){
####################
#initialization
####################
beta.old = beta0 #initiliaze beta
n = dim(X)[1]
p = dim(X)[2]
#Newon-Raphson
for(k in 1:epochs){
eta.hat = X %*% beta.old
y.hat = c(exp(eta.hat) / (1+exp(eta.hat))  )
w = pmax(y.hat*(1-y.hat) , 1e-10)  #numerical tolerance
X.tilde = sqrt(w)*X
y.tilde = eta.hat * w + (y-y.hat)
beta.new = c(solve(crossprod(X.tilde) +lambda*diag(c(0,rep(1,p-1))) ,
crossprod(X,y.tilde)))
if(max(abs(beta.new-beta.old)) < tol){ #no more updates
break
}
beta.old=beta.new
}
eta.hat = X %*% beta.old
y.hat = c(exp(eta.hat) /(1+exp(eta.hat))  )
pred = as.numeric(y.hat > 0.5)
class.err = 1-sum(pred==y)/n
return( list(coefficients=beta.new , y.hat=y.hat, pred=pred, err=class.err) )
}
logreg.ridge.cv(y,x, c(0,0.3,0.5),plot=T)
logreg.ridge.predict = function(logreg_object, X.new, y.new='None'){
beta = logreg_object[['coefficients']]
if(y.new[1] =='None'){
class.err='None'
n = dim(X.new)[1]
eta.hat = X.new %*% beta
y.hat = c(exp(eta.hat) / (1+exp(eta.hat)))
pred = as.numeric(y.hat>0.5)
}else{
n = length(y.new)
eta.hat = X.new %*% beta
y.hat = c( exp(eta.hat) ) / (1+exp(eta.hat))
pred = as.numeric(y.hat>0.5)
class.err = 1-sum(pred==y.new)/n
}
return( list(y.hat=y.hat, pred=pred, err=class.err) )
}
logreg.ridge.cv(y,x, c(0,0.3,0.5),plot=T)
#' logreg.ridge.cv
#' This function works to perform cross-validation to select the best lambda
#' @param y the input y as response
#' @param X the input X to fit model
#' @param lambdas a vector specifies the lambda candidates
#' @param nfolds an int sepecific how many folds you want to split the data
#' @param plot this boolean variable specifies if you want to plot the result
logreg.ridge.cv = function(y, X, lambdas, nfolds=5, plot=F){
n = dim(X)[1]
p = dim(X)[1]
num.lambda = length(lambdas)
beta.init = rep(0,p)
cv.results = rep(0,num.lambda)
for(i in 1:num.lambda){
groups = rep(1:nfolds, n/nfolds)
for( j in 1:nfolds){
train = groups!=j
test = groups==j
logreg_object = logreg.ridge.fit(y[train], X[train,] , lambda = lambdas[i],
epochs=10,tol=1e-3)
pred = logreg.ridge.predict(logreg_object, X[test,] ,y[test] )
cv.results[i] = cv.results[i] + pred$err/nfolds
}
}
print(cv.results)
print(lambdas)
if(plot){
plot( log(lambdas[i]) , cv.results, type='b',
xlab='log lambda', ylab='prediction error',
main=paste0(nfolds,"-fold CV results")
)
}
lambda.min =lambdas[cv.results==min(cv.results)]
return(list(lambda.min=lambda.min,err=cv.results,lambda.vec=lambdas))
}
logreg.ridge.cv(y,x, c(0,0.3,0.5),plot=T)
#' logreg.ridge.cv
#' This function works to perform cross-validation to select the best lambda
#' @param y the input y as response
#' @param X the input X to fit model
#' @param lambdas a vector specifies the lambda candidates
#' @param nfolds an int sepecific how many folds you want to split the data
#' @param plot this boolean variable specifies if you want to plot the result
logreg.ridge.cv = function(y, X, lambdas, nfolds=5, plot=F){
n = dim(X)[1]
p = dim(X)[1]
num.lambda = length(lambdas)
beta.init = rep(0,p)
cv.results = rep(0,num.lambda)
for(i in 1:num.lambda){
groups = rep(1:nfolds, n/nfolds)
for( j in 1:nfolds){
train = groups!=j
test = groups==j
logreg_object = logreg.ridge.fit(y[train], X[train,] , lambda = lambdas[i],
epochs=10,tol=1e-3)
pred = logreg.ridge.predict(logreg_object, X[test,] ,y[test] )
cv.results[i] = cv.results[i] + pred$err/nfolds
}
}
print(cv.results)
print(lambdas)
if(plot){
plot( log(lambdas) , cv.results, type='b',
xlab='log lambda', ylab='prediction error',
main=paste0(nfolds,"-fold CV results")
)
}
lambda.min =lambdas[cv.results==min(cv.results)]
return(list(lambda.min=lambda.min,err=cv.results,lambda.vec=lambdas))
}
logreg.ridge.cv(y,x, c(0,0.3,0.5),plot=T)
lambdas=c(0.1,0.004,0.7,4)
logreg.ridge.cv(y,x,lambdas,plot=T)
x=matrix(rnorm(100),20,5)
y=sample(c(0,1),20,replace=T)
lambdas=c(0.1,0.004,0.7,4)
lambdas=c(0.1,0.004,0.7,4)
y=sample(c(0,1),20,replace=T)
sample(c(0,1) , 20 , relapce=Tru# Fri Nov 27 17:45:02 2020 ------------------------------
)
sample(c(0,1) , 20 , relapce=True)
sample(c(0,1) , 20 , relapce=T)
sample(c(0,1) , 20 , replace=T)
sample.int(2,1-,replace=TRUE)
sample.int(2,10,replace=TRUE)
sample.int(1,10,replace=TRUE)
sample.int(c(0,1),10,replace=TRUE)
sample(c(0,1) , 20, replace=TRUE)
library(usethis)
usethis::user_vignette()
usethis::user_vignette()
usethis::user_vignette()
usethis::use_vignette()
usethis::use_vignette('tutorial.Rmd')
usethis::use_vignette('tutorial')
library(knitr)
usethis::use_vignette('tutorial')
usethis::use_vignette()
usethis::use_vignette('tutorial')
usethis::use_vignette("tutorial")
usethis:use_vignette("tutorial")
library(LogisticRidge)
install.packages(LogisticRidge)
library(LogisticRidge)
usethis::use_vignette("tutorial")
library(glmnet)
install.packages("glmnet")
x
y
glmnet(x , y, family='binomial', alpha=0, lambda=0)
library(glmnet)
glmnet(x , y, family='binomial', alpha=0, lambda=0)
summary( glmnet(x , y, family='binomial', alpha=0, lambda=0) )
glmnet(x , y, family='binomial', alpha=0, lambda=0)$beta
data("PimaIndiansDiabetes2", package = "mlbench")
data()
data("DNase")
Dnase
DNase
library(datasets)
data("iris")
iris
data(Ionosphere)
InsectSprays
airquality
iris
iris3
iris
iris[0:100,]
iris[:100,]
iris[1:100,]
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iric[1:100,]
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iris[1:100,]
isir_data
iris_data
glmnet(iris_data[,-ncol(iris_data)] , iris_data[,ncol(iris_data)], family='binomial')
unique(iris_data[ncol(iris_data)])
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iris[1:100,]
y = ifelse(iris_data=='versicolor', 1, 0)
y
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iris[1:100,]
y = ifelse(iris_data$Species =='versicolor', 1, 0)
X = iris_data[, -ncol(iris_data)]
X
glmnet(X, y , family='binomial', alpha=0 , lambda=0)
glmnet(as.matrix(X), y , family='binomial', alpha=0 , lambda=0)
a = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iris[1:100,]
y = ifelse(iris_data$Species =='versicolor', 1, 0)
X = as.matrix( iris_data[, -ncol(iris_data)] )
a = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
a$beta
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iris[1:100,]
y = ifelse(iris_data$Species =='versicolor', 1, 0)
X = as.matrix( iris_data[, -ncol(iris_data)] )
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
glm.object$beta
logreg.ridge.fit(y, X, lambda=0)
logreg.ridge.fit(y, X, lambda=0, epochs=20)
logreg.ridge.fit(y, X, lambda=0, epochs=20)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=30)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=5)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=1)$coefficients
glm.object$beta
logreg.ridge.fit(y, X, lambda=0, epochs=1)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=100)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=50)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=20)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=`0)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=`0)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=10)$coefficients
logreg.ridge.fit(y, X, lambda=0, epochs=200000)$coefficients
glm.object$lambda
glm.object$a0
glm.object$beta
X_new = cbind( 1, X )
X_new
logreg.ridge.fit(y, X_new, lambda=0, epochs=20)$coefficients
glm.object$beta
logreg.ridge.fit(y, X_new, lambda=0, epochs=10)$coefficients
logreg.ridge.fit(y, X_new, lambda=0, epochs=20)$coefficients
logreg.ridge.fit(y, X_new, lambda=0, epochs=20, tol=1e-3)$coefficients
logreg.ridge.fit(y, X_new, lambda=0, epochs=20, tol=1e-5)$coefficients
logreg.ridge.fit(y, X_new, lambda=0, epochs=200, tol=1e-3)$coefficients
logreg.ridge.fit(y, X_new, lambda=0, epochs=200, tol=1e-3)$coefficients
glm.object$a0
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
glm.object$beta
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
glm.object$beta
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
glm.object$beta
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0)
glm.object$beta
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0.5)
glm.object$beta
logreg.ridge.fit(y , nex_X)
logreg.ridge.fit(y , new_X)
logreg.ridge.fit(y , X_new)
logreg.ridge.fit(y , X_new, lambda=0.5)
logreg.ridge.fit(y , X_new, lambda=0.5)$coefficients
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0.5)
glm.object$beta
glm.object$a0
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0.5, intercept=FALSE)
glm.object$beta
logreg.ridge.fit(y , X_new, lambda=0.5)$coefficients
logreg.ridge.fit(y , X_new, lambda=0)$coefficients
glm.object = glmnet(X, y , family='binomial', alpha=0 , intercept=FALSE)
glm.object$beta
glm.object = glmnet(X, y , family='binomial', alpha=0 ,lambda=0, intercept=FALSE)
glm.object$beta
logreg.ridge.fit(y , X_new, lambda=0)$coefficients
glm.object$a0
logreg.ridge.fit(y , X, lambda=0)$coefficients
logreg.ridge.fit(y , X, lambda=0, epochs=30)$coefficients
glm.object = glmnet(X, y , family='binomial', alpha=0 ,lambda=0, intercept=FALSE, standardize=FALSE)
glm.object$beta
logreg.ridge.fit(y , X, lambda=0, epochs=10)$coefficients
logreg.ridge.fit(y , X, lambda=0, epochs=20)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=20)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=200)$coefficients
glm.object = glmnet(X, y , family='binomial', alpha=0 ,lambda=0, intercept=FALSE, standardize=FALSE, type.logistic='Newton')
glm.object$beta
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=200)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=20)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=10)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=15)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=14)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)$coefficients
library(LogisticRidge)
library(glmnet)
data("iris")
iris_data = iris[1:100,]
y = ifelse(iris_data$Species =='versicolor', 1, 0)
X = as.matrix( iris_data[, -ncol(iris_data)] )
glm.object = glmnet(X, y , family='binomial', alpha=0 , lambda=0, intercept=FALSE, standardize=FALSE)
glm.object$beta
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)$coefficients
logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)
predict.glm(glm.object , X)
predict.glmnet(glm.object , X)
predict.glmnetfit(glm.object , X)
predict(glm.object, X
)
glm.yhat = predict(glm.object, X)
y.hat = c(exp(glm.yhat) /(1+exp(glm.yhat))  )
pred = as.numeric(y.hat > 0.5)
glm.yhat = predict(glm.object, X)
y.hat = c(exp(glm.yhat) /(1+exp(glm.yhat))  )
pred = as.numeric(y.hat > 0.5)
pred
glm.yhat = predict(glm.object, X)
y.hat = c(exp(glm.yhat) /(1+exp(glm.yhat))  )
pred = as.numeric(y.hat > 0.5)
pred == logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)$pred
glm.yhat = predict(glm.object, X)
y.hat = c(exp(glm.yhat) /(1+exp(glm.yhat))  )
pred = as.numeric(y.hat > 0.5)
all( pred == logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)$pred )
X
logreg_object =  logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)
logreg_object
logreg_object =  logreg.ridge.fit(y , X, lambda=0, tol=1e-7, epochs=16)
logreg_object
logreg.ridge.predict(logreg_object, X , y  )
logreg.ridge.fit(y , X, lambda=3, tol=1e-7, epochs=16)
logreg.ridge.fit(y , X, lambda=100
, tol=1e-7, epochs=16)
logreg.ridge.fit(y , X, lambda=100000
, tol=1e-7, epochs=16)
logreg.ridge.fit(y , X, lambda=1000
, tol=1e-7, epochs=16)
logreg.ridge.fit(y , X, lambda=100
, tol=1e-7, epochs=16)
glmnet(X, y , family='binomial', alpha=0 , lambda=100, intercept=FALSE, standardize=FALSE)
glmnet(X, y , family='binomial', alpha=0 , lambda=100, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=100
, tol=1e-7, epochs=10)
logreg.ridge.fit(y , X, lambda=100
, tol=1e-7, epochs=10)$coefficients
logreg.ridge.fit(y , X, lambda=100
, tol=1e-7, epochs=10)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=100, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=10
, tol=1e-7, epochs=10)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=10
, tol=1e-7, epochs=100)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=10
, tol=1e-7, epochs=20)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=10
, tol=1e-3, epochs=20)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=10
, tol=1e-3, epochs=10)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X, lambda=20
, tol=1e-3, epochs=10)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
logreg.ridge.fit(y , X
, tol=1e-3, epochs=10)$coefficients
logreg.ridge.fit(y , X, lambda=10
, tol=1e-3, epochs=10)$coefficients
glmnet(X, y , family='binomial', alpha=0 , lambda=10, intercept=FALSE, standardize=FALSE)$beta
seed(123)
set.seed(123)
x=matrix(rnorm(100),20,5)
y=sample(c(0,1),20,replace=TRUE)
logreg.ridge.fit(y,x,lambda=0)
set.seed(123)
x=matrix(rnorm(100),20,5)
y=sample(c(0,1),20,replace=TRUE)
logreg.ridge.fit(y,x,lambda=0)
glmnet(x, y , family='binomial', alpha=0 , lambda=0, intercept=FALSE, standardize=FALSE)$beta
lambdas = c(0,0.3,0.7,1.2,100)
logreg.ridge.cv(y, x, lambdas= lambdas)
cv.glmnet(x, y, alpha = 0, family = "binomial")
warnings()
cv.glmnet(x, y, alpha = 0, family = "binomial", lambda=lambdas)
lambdas = c(0.01,0.3,0.7,1.2,100)
logreg.ridge.cv(y, x, lambdas= lambdas)
lambdas = c(0.1,0.3,0.7,1.2,100)
logreg.ridge.cv(y, x, lambdas= lambdas)
lambdas = c(0.1,0.3,0.7,1.2,10000)
logreg.ridge.cv(y, x, lambdas= lambdas)
set.seed(123)
x=matrix(rnorm(100),200,5)
y=sample(c(0,1),200,replace=TRUE)
logreg.ridge.fit(y,x,lambda=0)
set.seed(123)
x=matrix(rnorm(100),20,5)
y=sample(c(0,1),20,replace=TRUE)
logreg.ridge.fit(y,x,lambda=0)
glmnet(x, y , family='binomial', alpha=0 , lambda=0, intercept=FALSE, standardize=FALSE)$beta
lambdas = c(0.1,0.3,0.7,1.2,0)
logreg.ridge.cv(y, x, lambdas= lambdas)
devtools::document()
library(LogisticRidge)
